{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from geo.drift_compensation import get_track_drift_rate\n",
    "from get_turn import get_turning_points, plot_changepoints, TurnAndRise, write_turnandrise_to_zarr\n",
    "import csv\n",
    "import random\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs() -> None:\n",
    "    # Make /workspace/deepflow/data/osstate/dangling if it doesn't exist\n",
    "    os.makedirs('/workspace/deepflow/data/osstate/dangling', exist_ok=True)\n",
    "\n",
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_filepath(filepath: str) -> str:\n",
    "    # Get the filename without the extension, which is everything after the last slash and before the first period\n",
    "    if '/' in filepath:\n",
    "        filename = filepath.split('/')[-1].split('.')[0]\n",
    "    else:\n",
    "        filename = filepath.split('.')[0]\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(filepath: str, logtag: str, n_idents: int = 0, ident_filter: List[str] = [], ident_mandatory: List[str] = []):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(logtag)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh = logging.FileHandler(f'{logtag}.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Read the file and preprocess it\n",
    "    df:pd.DataFrame = pd.read_csv(filepath, compression='gzip')\n",
    "    logger.info(f\"Read {len(df)} rows from {filepath}\")\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    logger.info(f\"Dropped NaN rows, {len(df)} rows remaining\")\n",
    "    # add an ident column by concatenating df['callsign'] and df['icao24']\n",
    "    df['ident'] = (df['callsign'].str.strip()+'_'+df['icao24'].str.strip())\n",
    "    # add a column rtime that is df['time'] - df['time'].min()\n",
    "    df['rtime'] = df['time'] - df['time'].min()\n",
    "    # Drop the columns we don't need\n",
    "    df.drop(columns=['onground', 'alert', 'spi', 'squawk'], inplace=True)\n",
    "    idents = df['ident'].unique()\n",
    "    # If n_idents is greater than the number of unique idents, set n_idents to the number of unique idents\n",
    "    if len(idents) < n_idents:\n",
    "        n_idents = len(idents)\n",
    "\n",
    "    # Only keep the idents that are in the ident_filter\n",
    "    if len(ident_filter) > 0:\n",
    "        idents = [ident for ident in idents if ident in ident_filter]\n",
    "    \n",
    "    # Add the mandatory idents to the list by finding intersection between idents and ident_mandatory\n",
    "    if len(ident_mandatory) > 0:\n",
    "        ident_mandatory_collected = [ident for ident in idents if ident in ident_mandatory] # idents that are in both idents and ident_mandatory\n",
    "        \n",
    "        if len(ident_mandatory_collected) < len(ident_mandatory):\n",
    "            print(f\"Could not find all mandatory idents\")\n",
    "            print(f\"Found only {len(ident_mandatory_collected)} out of {len(ident_mandatory)} mandatory idents\")\n",
    "            logger.error(f\"Could not find all mandatory idents: {ident_mandatory}\")\n",
    "\n",
    "        # Keep ident_mandatory_collected and add random idents to the list to make up n_idents\n",
    "        if n_idents > 0 and len(ident_mandatory) < n_idents:\n",
    "            new_idents_pool = [ident for ident in idents if ident not in ident_mandatory_collected] # pool of idents to choose from for the remaining slots\n",
    "            new_idents = random.sample(new_idents_pool, n_idents - len(ident_mandatory)) # Choose n_idents - len(ident_mandatory) random idents\n",
    "            new_idents = list(set(new_idents)) # Remove duplicates\n",
    "            idents = ident_mandatory_collected + new_idents\n",
    "        elif n_idents > 0: # len(ident_mandatory) >= n_idents: too many mandatory idents, keep only first n_idents\n",
    "            idents = ident_mandatory_collected[:n_idents]\n",
    "        else:\n",
    "            raise ValueError(\"n_idents must be greater than 0\")\n",
    "\n",
    "    else: # len(ident_mandatory) == 0 or no ident_mandatory specified\n",
    "        if n_idents > 0:\n",
    "            idents = random.sample(list(idents), n_idents)\n",
    "        else:\n",
    "            idents = list(idents)\n",
    "    \n",
    "    logger.info(f\"Processing {len(idents)} unique idents\")\n",
    "\n",
    "    filename = get_filename_from_filepath(filepath)\n",
    "\n",
    "    # Create a folder called filename inside the routes folder\n",
    "    os.makedirs(f'/workspace/deepflow/data/osstate/routes/{filename}', exist_ok=True)\n",
    "\n",
    "    # To write dangling flights to a separate CSV file\n",
    "    with open(f'/workspace/deepflow/data/osstate/dangling/{filename}.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['filename', 'ident'])\n",
    "        for ident in idents:\n",
    "            try:\n",
    "                # Get the subdf for the ident\n",
    "                df_ident = df[df['ident'] == ident]\n",
    "                if len(df_ident) == 0:\n",
    "                    logger.error(f\"Ident {ident} not found in the dataframe\")\n",
    "                    continue\n",
    "                turns:TurnAndRise = get_turning_points(df_ident)\n",
    "                if not turns['landed']:\n",
    "                    # Aircraft not yet landed, write to the dangling CSV file\n",
    "                    \n",
    "                    writer.writerow([filename, ident])\n",
    "                \n",
    "                write_turnandrise_to_zarr(turns, f'/workspace/deepflow/data/osstate/routes/{filename}/{ident}.zarr')\n",
    "                logger.info(f\"Processed {ident}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {ident}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dangling_idents(filepath: str) -> List[str]:\n",
    "    filename = get_filename_from_filepath(filepath)\n",
    "    try:\n",
    "        dangling_df = pd.read_csv(f'/workspace/deepflow/data/osstate/dangling/{filename}.csv')\n",
    "        return dangling_df['ident'].unique().tolist()\n",
    "    except FileNotFoundError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file_list() -> List[str]:\n",
    "    # List all the files in the data folder\n",
    "    data_files = os.listdir('/workspace/deepflow/data/osstate/extracted')\n",
    "    # Only keep the .csv.gz files\n",
    "    data_files = [file for file in data_files if file.endswith('.csv.gz')]\n",
    "    data_files = [f'/workspace/deepflow/data/osstate/extracted/{file}' for file in data_files]\n",
    "    # Sort the files alphabetically\n",
    "    data_files.sort()\n",
    "    print(f'Found {len(data_files)} files')\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test before multiprocessing\n",
    "\n",
    "# file_list = get_data_file_list()\n",
    "# process_csv_file(file_list[0], 'test', n_idents=100, ident_mandatory=[])\n",
    "# print('Dangling idents: ', get_dangling_idents(file_list[0]))\n",
    "# process_csv_file(filepath=file_list[1], logtag='test2', n_idents=100, ident_mandatory=get_dangling_idents(file_list[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 files\n",
      "Processing 24 files in thread 0Processing 24 files in thread 1Processing 24 files in thread 2\n",
      "\n",
      "Processing 24 files in thread 3\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_file(file_list, thread_number, n_idents = 2000):\n",
    "    print(f\"Processing {len(file_list)} files in thread {thread_number}\")\n",
    "    for index, file in enumerate(file_list):\n",
    "        if index == 0:\n",
    "            process_csv_file(filepath=file, logtag=file, n_idents=n_idents)\n",
    "        else:\n",
    "            process_csv_file(filepath=file, logtag=file, n_idents=n_idents, ident_mandatory=get_dangling_idents(file_list[index - 1]))\n",
    "\n",
    "do_not_allow_delete = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_list = get_data_file_list()\n",
    "    num_processes = 4\n",
    "    processes = []\n",
    "\n",
    "    # Divide the file list into num_processes chunks\n",
    "    file_list = [file_list[i:i + len(file_list) // num_processes] for i in range(0, len(file_list), len(file_list) // num_processes)]\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        p = mp.Process(target=process_file, args=(file_list[i], i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUTION: DELETE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wipe_slate():\n",
    "    # Wipe the slate clean\n",
    "    !rm -rf /workspace/deepflow/data/osstate/routes/*\n",
    "    !rm -rf /workspace/deepflow/data/osstate/dangling/*\n",
    "    !rm -rf /workspace/deepflow/data/osstate/dangling/.ipynb_checkpoints/*\n",
    "    !rm -rf /workspace/deepflow/data/osstate/dangling/.ipynb_checkpoints\n",
    "    !rm -rf /workspace/deepflow/data/osstate/extracted/*.log\n",
    "    !rm -rf *.log\n",
    "    !rm -rf /workspace/deepflow/data/osstate/waypoints_master.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/workspace/deepflow/data/osstate/routes/states_2022-04-04-00/UAL1651_aa56b8.zarr': Directory not empty\n",
      "rm: cannot remove '/workspace/deepflow/data/osstate/routes/states_2022-04-11-00/SWA1576_abc7e9.zarr': Directory not empty\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "wipe_slate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
