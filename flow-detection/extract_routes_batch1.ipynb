{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_prefix import PATH_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from geo.drift_compensation import get_track_drift_rate\n",
    "from get_turn import get_turning_points, plot_changepoints, TurnAndRise, write_turnandrise_to_zarr\n",
    "import csv\n",
    "import random\n",
    "from typing import List\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /Users/thinhhoang/Documents/XFD/data/osstate/dangling\n",
      "Created /Users/thinhhoang/Documents/XFD/data/osstate/routes\n"
     ]
    }
   ],
   "source": [
    "def make_dirs() -> None:\n",
    "    # Make /workspace/deepflow/data/osstate/dangling if it doesn't exist\n",
    "    os.makedirs(f'{PATH_PREFIX}/data/osstate/dangling', exist_ok=True)\n",
    "    os.makedirs(f'{PATH_PREFIX}/data/osstate/routes', exist_ok=True)\n",
    "    print(f'Created {PATH_PREFIX}/data/osstate/dangling')\n",
    "    print(f'Created {PATH_PREFIX}/data/osstate/routes')\n",
    "\n",
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_string(length):\n",
    "    \"\"\"\n",
    "    Generate a random string of characters and numbers with the specified length.\n",
    "    \n",
    "    Args:\n",
    "    length (int): The desired length of the random string.\n",
    "    \n",
    "    Returns:\n",
    "    str: A random string of characters and numbers.\n",
    "    \"\"\"\n",
    "    # Define the character set: lowercase letters, uppercase letters, and digits\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    \n",
    "    # Generate the random string\n",
    "    random_string = ''.join(random.choice(characters) for _ in range(length))\n",
    "    \n",
    "    return random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_filepath(filepath: str) -> str:\n",
    "    # Get the filename without the extension, which is everything after the last slash and before the first period\n",
    "    if '/' in filepath:\n",
    "        filename = filepath.split('/')[-1].split('.')[0]\n",
    "    else:\n",
    "        filename = filepath.split('.')[0]\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(filepath: str, logtag: str, n_idents: int = 0, ident_filter: List[str] = [], ident_mandatory: List[str] = []):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(logtag)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh = logging.FileHandler(f'{logtag}.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Read the file and preprocess it\n",
    "    df:pd.DataFrame = pd.read_csv(filepath, compression='gzip')\n",
    "    logger.info(f\"Read {len(df)} rows from {filepath}\")\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    logger.info(f\"Dropped NaN rows, {len(df)} rows remaining\")\n",
    "    # add an ident column by concatenating df['callsign'] and df['icao24']\n",
    "    df['ident'] = (df['callsign'].str.strip()+'_'+df['icao24'].str.strip())\n",
    "    # add a column rtime that is df['time'] - df['time'].min()\n",
    "    df['rtime'] = df['time'] - df['time'].min()\n",
    "    # Drop the columns we don't need\n",
    "    df.drop(columns=['onground', 'alert', 'spi', 'squawk'], inplace=True)\n",
    "    idents = df['ident'].unique()\n",
    "    # If n_idents is greater than the number of unique idents, set n_idents to the number of unique idents\n",
    "    if len(idents) < n_idents:\n",
    "        n_idents = len(idents)\n",
    "\n",
    "    # Only keep the idents that are in the ident_filter\n",
    "    if len(ident_filter) > 0:\n",
    "        idents = [ident for ident in idents if ident in ident_filter]\n",
    "    \n",
    "    # Add the mandatory idents to the list by finding intersection between idents and ident_mandatory\n",
    "    if len(ident_mandatory) > 0:\n",
    "        ident_mandatory_collected = [ident for ident in idents if ident in ident_mandatory] # idents that are in both idents and ident_mandatory\n",
    "        \n",
    "        if len(ident_mandatory_collected) < len(ident_mandatory):\n",
    "            print(f\"Could not find all mandatory idents\")\n",
    "            print(f\"Found only {len(ident_mandatory_collected)} out of {len(ident_mandatory)} mandatory idents\")\n",
    "            logger.error(f\"Could not find all mandatory idents: {ident_mandatory}\")\n",
    "\n",
    "        # Keep ident_mandatory_collected and add random idents to the list to make up n_idents\n",
    "        if n_idents > 0 and len(ident_mandatory) < n_idents:\n",
    "            new_idents_pool = [ident for ident in idents if ident not in ident_mandatory_collected] # pool of idents to choose from for the remaining slots\n",
    "            new_idents = random.sample(new_idents_pool, n_idents - len(ident_mandatory)) # Choose n_idents - len(ident_mandatory) random idents\n",
    "            new_idents = list(set(new_idents)) # Remove duplicates\n",
    "            idents = ident_mandatory_collected + new_idents\n",
    "        elif n_idents > 0: # len(ident_mandatory) >= n_idents: too many mandatory idents, keep only first n_idents\n",
    "            idents = ident_mandatory_collected[:n_idents]\n",
    "        else:\n",
    "            raise ValueError(\"n_idents must be greater than 0\")\n",
    "\n",
    "    else: # len(ident_mandatory) == 0 or no ident_mandatory specified\n",
    "        if n_idents > 0:\n",
    "            idents = random.sample(list(idents), n_idents)\n",
    "        else:\n",
    "            idents = list(idents)\n",
    "    \n",
    "    logger.info(f\"Processing {len(idents)} unique idents\")\n",
    "\n",
    "    filename = get_filename_from_filepath(filepath)\n",
    "\n",
    "    # Create a folder called filename inside the routes folder\n",
    "    # os.makedirs(f'{PATH_PREFIX}/data/osstate/routes/{filename}', exist_ok=True)\n",
    "\n",
    "    # File objects for writing to CSV: dangling_file, route_file for turning points - altitude change points, links_file, waypoints_file\n",
    "    dangling_file = open(f'{PATH_PREFIX}/data/osstate/dangling/{filename}.danglings', 'w', newline='')\n",
    "    # TP: turning points, DP: altitude change points\n",
    "    route_file_tp = open(f'{PATH_PREFIX}/data/osstate/routes/{filename}.turns', 'w', newline='')\n",
    "    route_file_dp = open(f'{PATH_PREFIX}/data/osstate/routes/{filename}.climbs', 'w', newline='')\n",
    "    links_file = open(f'{PATH_PREFIX}/data/osstate/routes/{filename}.links', 'w', newline='')\n",
    "    waypoints_file = open(f'{PATH_PREFIX}/data/osstate/routes/{filename}.waypoints', 'w', newline='')\n",
    "\n",
    "    # Create the CSV writers\n",
    "    dangling_writer = csv.writer(dangling_file)\n",
    "    # The header of dangling file\n",
    "    dangling_writer.writerow(['filename', 'ident'])\n",
    "\n",
    "    tp_route_writer = csv.writer(route_file_tp)\n",
    "    dp_route_writer = csv.writer(route_file_dp)\n",
    "    # The header of route files\n",
    "    tp_route_writer.writerow(['ident','wp','time','lat','lon','alt','vel'])\n",
    "    dp_route_writer.writerow(['ident','time','lat','lon','alt','vel']) # the altitude change points don't have a waypoint name\n",
    "    \n",
    "    waypoints_writer = csv.writer(waypoints_file)\n",
    "    waypoints_writer.writerow(['wp','lat','lon','ident'])\n",
    "\n",
    "    links_writer = csv.writer(links_file)\n",
    "    links_writer.writerow(['link','wp1','wp2','time1','time2','lat1','lon1','lat2','lon2','ident'])\n",
    "    \n",
    "    for ident in idents:\n",
    "        try:\n",
    "\n",
    "            # Get the subdf for the ident\n",
    "            df_ident = df[df['ident'] == ident]\n",
    "            if len(df_ident) == 0:\n",
    "                logger.error(f\"Ident {ident} not found in the dataframe\")\n",
    "                continue\n",
    "            turns:TurnAndRise = get_turning_points(df_ident)\n",
    "            # Write the turning points and altitude change points to the CSV files\n",
    "            last_wp_name = '' # to store the last waypoint name, useful to write the links\n",
    "            for i in range(len(turns['tp_time'])):\n",
    "                wp_name:str = 'W' + generate_random_string(24) # Generate a random string for the waypoint name, 24 characters long starting with W\n",
    "                tp_route_writer.writerow([ident, wp_name, turns['tp_time'][i], turns['tp_lat'][i], turns['tp_lon'][i], turns['tp_alt'][i], turns['tp_vel'][i]])\n",
    "                waypoints_writer.writerow([wp_name, turns['tp_lat'][i], turns['tp_lon'][i], ident])\n",
    "                if last_wp_name != '':\n",
    "                    link_name:str = 'L' + generate_random_string(24) # Generate a random string for the link name, 24 characters long starting with L\n",
    "                    links_writer.writerow([link_name, last_wp_name, wp_name, turns['tp_time'][i-1], turns['tp_time'][i], turns['tp_lat'][i-1], turns['tp_lon'][i-1], turns['tp_lat'][i], turns['tp_lon'][i], ident])\n",
    "                last_wp_name = wp_name\n",
    "            for i in range(len(turns['dp_time'])):\n",
    "                dp_route_writer.writerow([ident, turns['dp_time'][i], turns['dp_lat'][i], turns['dp_lon'][i], turns['dp_alt'][i], turns['dp_vel'][i]])\n",
    "\n",
    "            if not turns['landed']:\n",
    "                # Aircraft not yet landed, write to the dangling CSV file\n",
    "                dangling_writer.writerow([filename, ident])\n",
    "            # write_turnandrise_to_zarr(turns, f'{PATH_PREFIX}/data/osstate/routes/{filename}/{ident}.zarr')\n",
    "            logger.info(f\"Processed {ident}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {ident}: {e}\")\n",
    "    \n",
    "    # Close the file objects\n",
    "    dangling_file.close()\n",
    "    route_file_tp.close()\n",
    "    route_file_dp.close()\n",
    "    links_file.close()\n",
    "    waypoints_file.close()\n",
    "    logger.info(f\"Finished processing {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dangling_idents(filepath: str) -> List[str]:\n",
    "    filename = get_filename_from_filepath(filepath)\n",
    "    try:\n",
    "        dangling_df = pd.read_csv(f'{PATH_PREFIX}/data/osstate/dangling/{filename}.danglings')\n",
    "        return dangling_df['ident'].unique().tolist()\n",
    "    except FileNotFoundError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file_list() -> List[str]:\n",
    "    # List all the files in the data folder\n",
    "    data_files = os.listdir(f'{PATH_PREFIX}/data/osstate/extracted')\n",
    "    # Only keep the .csv.gz files\n",
    "    data_files = [file for file in data_files if file.endswith('.csv.gz')]\n",
    "    data_files = [f'{PATH_PREFIX}/data/osstate/extracted/{file}' for file in data_files]\n",
    "    # Sort the files alphabetically\n",
    "    data_files.sort()\n",
    "    print(f'Found {len(data_files)} files')\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test before multiprocessing\n",
    "\n",
    "# file_list = get_data_file_list()\n",
    "# process_csv_file(file_list[0], 'test', n_idents=100, ident_mandatory=[])\n",
    "# print('Dangling idents: ', get_dangling_idents(file_list[0]))\n",
    "# process_csv_file(filepath=file_list[1], logtag='test2', n_idents=100, ident_mandatory=get_dangling_idents(file_list[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files\n",
      "<function process_file at 0x10c5ed440>\n",
      "Processing 1 files in thread 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deepflow/lib/python3.12/site-packages/multiprocess/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deepflow/lib/python3.12/site-packages/multiprocess/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deepflow/lib/python3.12/site-packages/multiprocess/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_file(file_list, thread_number, n_idents = 2000):\n",
    "    print(f\"Processing {len(file_list)} files in thread {thread_number}\")\n",
    "    for index, file in enumerate(file_list):\n",
    "        if index == 0:\n",
    "            process_csv_file(filepath=file, logtag=file, n_idents=n_idents)\n",
    "        else:\n",
    "            process_csv_file(filepath=file, logtag=file, n_idents=n_idents, ident_mandatory=get_dangling_idents(file_list[index - 1]))\n",
    "\n",
    "do_not_allow_delete = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_list = get_data_file_list()\n",
    "    num_processes = min(mp.cpu_count(), len(file_list))\n",
    "    processes = []\n",
    "\n",
    "    # Divide the file list into num_processes chunks\n",
    "    file_list = [file_list[i:i + len(file_list) // num_processes] for i in range(0, len(file_list), len(file_list) // num_processes)]\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        print(process_file)\n",
    "        p = mp.Process(target=process_file, args=(file_list[i], i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUTION: DELETE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wipe_slate():\n",
    "    # Wipe the slate clean\n",
    "    !rm -rf {PATH_PREFIX}/data/osstate/dangling/*\n",
    "    !rm -rf {PATH_PREFIX}/data/osstate/routes/*\n",
    "    !rm -rf {PATH_PREFIX}/data/osstate/extracted/*.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wipe_slate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
