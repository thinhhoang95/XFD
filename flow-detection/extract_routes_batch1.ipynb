{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_prefix import PATH_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from geo.drift_compensation import get_track_drift_rate\n",
    "from get_turn import get_turning_points, plot_changepoints, TurnAndRise, write_turnandrise_to_zarr\n",
    "import csv\n",
    "import random\n",
    "from typing import List\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /Users/thinhhoang/Documents/XFD/data/osstate/dangling\n"
     ]
    }
   ],
   "source": [
    "def make_dirs() -> None:\n",
    "    # Make /workspace/deepflow/data/osstate/dangling if it doesn't exist\n",
    "    os.makedirs(f'{PATH_PREFIX}/data/osstate/dangling', exist_ok=True)\n",
    "    print(f'Created {PATH_PREFIX}/data/osstate/dangling')\n",
    "\n",
    "make_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_filepath(filepath: str) -> str:\n",
    "    # Get the filename without the extension, which is everything after the last slash and before the first period\n",
    "    if '/' in filepath:\n",
    "        filename = filepath.split('/')[-1].split('.')[0]\n",
    "    else:\n",
    "        filename = filepath.split('.')[0]\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(filepath: str, logtag: str, n_idents: int = 0, ident_filter: List[str] = [], ident_mandatory: List[str] = []):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(logtag)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh = logging.FileHandler(f'{logtag}.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Read the file and preprocess it\n",
    "    df:pd.DataFrame = pd.read_csv(filepath, compression='gzip')\n",
    "    logger.info(f\"Read {len(df)} rows from {filepath}\")\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    logger.info(f\"Dropped NaN rows, {len(df)} rows remaining\")\n",
    "    # add an ident column by concatenating df['callsign'] and df['icao24']\n",
    "    df['ident'] = (df['callsign'].str.strip()+'_'+df['icao24'].str.strip())\n",
    "    # add a column rtime that is df['time'] - df['time'].min()\n",
    "    df['rtime'] = df['time'] - df['time'].min()\n",
    "    # Drop the columns we don't need\n",
    "    df.drop(columns=['onground', 'alert', 'spi', 'squawk'], inplace=True)\n",
    "    idents = df['ident'].unique()\n",
    "    # If n_idents is greater than the number of unique idents, set n_idents to the number of unique idents\n",
    "    if len(idents) < n_idents:\n",
    "        n_idents = len(idents)\n",
    "\n",
    "    # Only keep the idents that are in the ident_filter\n",
    "    if len(ident_filter) > 0:\n",
    "        idents = [ident for ident in idents if ident in ident_filter]\n",
    "    \n",
    "    # Add the mandatory idents to the list by finding intersection between idents and ident_mandatory\n",
    "    if len(ident_mandatory) > 0:\n",
    "        ident_mandatory_collected = [ident for ident in idents if ident in ident_mandatory] # idents that are in both idents and ident_mandatory\n",
    "        \n",
    "        if len(ident_mandatory_collected) < len(ident_mandatory):\n",
    "            print(f\"Could not find all mandatory idents\")\n",
    "            print(f\"Found only {len(ident_mandatory_collected)} out of {len(ident_mandatory)} mandatory idents\")\n",
    "            logger.error(f\"Could not find all mandatory idents: {ident_mandatory}\")\n",
    "\n",
    "        # Keep ident_mandatory_collected and add random idents to the list to make up n_idents\n",
    "        if n_idents > 0 and len(ident_mandatory) < n_idents:\n",
    "            new_idents_pool = [ident for ident in idents if ident not in ident_mandatory_collected] # pool of idents to choose from for the remaining slots\n",
    "            new_idents = random.sample(new_idents_pool, n_idents - len(ident_mandatory)) # Choose n_idents - len(ident_mandatory) random idents\n",
    "            new_idents = list(set(new_idents)) # Remove duplicates\n",
    "            idents = ident_mandatory_collected + new_idents\n",
    "        elif n_idents > 0: # len(ident_mandatory) >= n_idents: too many mandatory idents, keep only first n_idents\n",
    "            idents = ident_mandatory_collected[:n_idents]\n",
    "        else:\n",
    "            raise ValueError(\"n_idents must be greater than 0\")\n",
    "\n",
    "    else: # len(ident_mandatory) == 0 or no ident_mandatory specified\n",
    "        if n_idents > 0:\n",
    "            idents = random.sample(list(idents), n_idents)\n",
    "        else:\n",
    "            idents = list(idents)\n",
    "    \n",
    "    logger.info(f\"Processing {len(idents)} unique idents\")\n",
    "\n",
    "    filename = get_filename_from_filepath(filepath)\n",
    "\n",
    "    # Create a folder called filename inside the routes folder\n",
    "    os.makedirs(f'{PATH_PREFIX}/data/osstate/routes/{filename}', exist_ok=True)\n",
    "\n",
    "    # To write dangling flights to a separate CSV file\n",
    "    with open(f'{PATH_PREFIX}/data/osstate/dangling/{filename}.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['filename', 'ident'])\n",
    "        for ident in idents:\n",
    "            try:\n",
    "                # Get the subdf for the ident\n",
    "                df_ident = df[df['ident'] == ident]\n",
    "                if len(df_ident) == 0:\n",
    "                    logger.error(f\"Ident {ident} not found in the dataframe\")\n",
    "                    continue\n",
    "                turns:TurnAndRise = get_turning_points(df_ident)\n",
    "                if not turns['landed']:\n",
    "                    # Aircraft not yet landed, write to the dangling CSV file\n",
    "                    \n",
    "                    writer.writerow([filename, ident])\n",
    "                \n",
    "                write_turnandrise_to_zarr(turns, f'{PATH_PREFIX}/data/osstate/routes/{filename}/{ident}.zarr')\n",
    "                logger.info(f\"Processed {ident}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {ident}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dangling_idents(filepath: str) -> List[str]:\n",
    "    filename = get_filename_from_filepath(filepath)\n",
    "    try:\n",
    "        dangling_df = pd.read_csv(f'{PATH_PREFIX}/data/osstate/dangling/{filename}.csv')\n",
    "        return dangling_df['ident'].unique().tolist()\n",
    "    except FileNotFoundError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file_list() -> List[str]:\n",
    "    # List all the files in the data folder\n",
    "    data_files = os.listdir(f'{PATH_PREFIX}/data/osstate/extracted')\n",
    "    # Only keep the .csv.gz files\n",
    "    data_files = [file for file in data_files if file.endswith('.csv.gz')]\n",
    "    data_files = [f'{PATH_PREFIX}/data/osstate/extracted/{file}' for file in data_files]\n",
    "    # Sort the files alphabetically\n",
    "    data_files.sort()\n",
    "    print(f'Found {len(data_files)} files')\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test before multiprocessing\n",
    "\n",
    "# file_list = get_data_file_list()\n",
    "# process_csv_file(file_list[0], 'test', n_idents=100, ident_mandatory=[])\n",
    "# print('Dangling idents: ', get_dangling_idents(file_list[0]))\n",
    "# process_csv_file(filepath=file_list[1], logtag='test2', n_idents=100, ident_mandatory=get_dangling_idents(file_list[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files\n",
      "<function process_file at 0x13e70bd80>\n",
      "Processing 1 files in thread 0\n"
     ]
    }
   ],
   "source": [
    "def process_file(file_list, thread_number, n_idents = 2000):\n",
    "    print(f\"Processing {len(file_list)} files in thread {thread_number}\")\n",
    "    for index, file in enumerate(file_list):\n",
    "        if index == 0:\n",
    "            process_csv_file(filepath=file, logtag=file, n_idents=n_idents)\n",
    "        else:\n",
    "            process_csv_file(filepath=file, logtag=file, n_idents=n_idents, ident_mandatory=get_dangling_idents(file_list[index - 1]))\n",
    "\n",
    "do_not_allow_delete = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_list = get_data_file_list()\n",
    "    num_processes = min(mp.cpu_count(), len(file_list))\n",
    "    processes = []\n",
    "\n",
    "    # Divide the file list into num_processes chunks\n",
    "    file_list = [file_list[i:i + len(file_list) // num_processes] for i in range(0, len(file_list), len(file_list) // num_processes)]\n",
    "\n",
    "    for i in range(num_processes):\n",
    "        print(process_file)\n",
    "        p = mp.Process(target=process_file, args=(file_list[i], i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUTION: DELETE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wipe_slate():\n",
    "    # Wipe the slate clean\n",
    "    !rm -rf /Volumes/CrucialX/deepflow/data/osstate/routes/*\n",
    "    !rm -rf /Volumes/CrucialX/deepflow/data/osstate/dangling/*\n",
    "    !rm -rf /Volumes/CrucialX/deepflow/data/osstate/dangling/.ipynb_checkpoints/*\n",
    "    !rm -rf /Volumes/CrucialX/deepflow/data/osstate/dangling/.ipynb_checkpoints\n",
    "    !rm -rf /Volumes/CrucialX/deepflow/data/osstate/extracted/*.log\n",
    "    !rm -rf *.log\n",
    "    !rm -rf /Volumes/CrucialX/deepflow/data/osstate/waypoints_master.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wipe_slate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
