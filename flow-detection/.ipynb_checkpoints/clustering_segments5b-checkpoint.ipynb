{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a `152 bit` version of the hash code, so it allows tighter clustering of the flows.\n",
    "\n",
    "The code is essentially similar to the file `clustering_desgments5.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create `.hsb` file for each flight route, and the `hash_master.hashb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H882U2Xn5VOd</td>\n",
       "      <td>50.404884</td>\n",
       "      <td>-98.626030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P8ToOqoOwy3c</td>\n",
       "      <td>32.375521</td>\n",
       "      <td>130.663520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oZYSGKN80id2</td>\n",
       "      <td>41.086054</td>\n",
       "      <td>-77.855456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uPuRzZYavxw1</td>\n",
       "      <td>40.908554</td>\n",
       "      <td>-78.007263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lEf9PODPqPxe</td>\n",
       "      <td>40.573301</td>\n",
       "      <td>-78.509674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ident        lat         lon\n",
       "0  H882U2Xn5VOd  50.404884  -98.626030\n",
       "1  P8ToOqoOwy3c  32.375521  130.663520\n",
       "2  oZYSGKN80id2  41.086054  -77.855456\n",
       "3  uPuRzZYavxw1  40.908554  -78.007263\n",
       "4  lEf9PODPqPxe  40.573301  -78.509674"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open a waypoint_master file\n",
    "waypoints_master_df = pd.read_csv('../data/osstate/waypoints_master.csv')\n",
    "waypoints_master_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "\n",
    "def viz_wp(lats: np.ndarray, lons: np.ndarray) -> None:\n",
    "\n",
    "    # Create a figure and axes with a specific projection\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "    # Add country borders for reference\n",
    "    ax.add_feature(cartopy.feature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cartopy.feature.COASTLINE)\n",
    "    ax.add_feature(cartopy.feature.LAND, edgecolor='black')\n",
    "\n",
    "    # Plot the point\n",
    "    for i in range(len(lats)):\n",
    "        ax.plot(lons[i], lats[i], 'ro', markersize=10, transform=ccrs.PlateCarree())\n",
    "        ax.text(lons[i], lats[i], f'WP {i}', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Set the extent of the map to the area around the point\n",
    "    #longitude = np.min(lons)\n",
    "    #latitude = np.min(lats)\n",
    "    #ax.set_extent([longitude-5, longitude+5, latitude-5, latitude+5])\n",
    "\n",
    "    # Add gridlines\n",
    "    ax.gridlines()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hash_segments import hash_with_hpp, compute_segment_position_hash, latlon2xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_file_extension(filename: str) -> str:\n",
    "    # Get the filename, which is everything before the first period and after the last slash\n",
    "    filename = filename.split('/')[-1]\n",
    "    filename = filename.split('.')[0]\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_vectors(wp_lats: np.ndarray, wp_lon: np.ndarray) -> np.ndarray:\n",
    "    # Convert the lats and lons to xyz\n",
    "    wp_xyz = latlon2xyz(wp_lats, wp_lon).T # n waypoints x 3\n",
    "    segment_start = wp_xyz[:-1]\n",
    "    segment_end = wp_xyz[1:]\n",
    "    wp_normals = np.cross(segment_start, segment_end)\n",
    "    # Normalize the normals\n",
    "    wp_normals /= np.linalg.norm(wp_normals, axis=1)[:, np.newaxis]\n",
    "    # Fix the orientation of the normals\n",
    "    wp_normals = np.where(wp_normals[:, 2][:, np.newaxis] >= 0, wp_normals, -wp_normals)\n",
    "    return wp_normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_midpoints(wp_lats: np.ndarray, wp_lon: np.ndarray) -> np.ndarray:\n",
    "    segment_start_lats = wp_lats[:-1]\n",
    "    segment_start_lons = wp_lon[:-1]\n",
    "    segment_end_lats = wp_lats[1:]\n",
    "    segment_end_lons = wp_lon[1:]\n",
    "    midpoints_lats = (segment_start_lats + segment_end_lats) / 2\n",
    "    midpoints_lons = (segment_start_lons + segment_end_lons) / 2\n",
    "    return midpoints_lats, midpoints_lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the ../data/osstate/routes_hashes/hash_master.hash file if it exists\n",
    "!rm -rf ../data/osstate/routes_hashes/*.hsb\n",
    "!rm -rf ../data/osstate/routes_hashes/hash_master.hashb\n",
    "\n",
    "# Create a hash_master file\n",
    "with open('../data/osstate/routes_hashes/hash_master.hashb', 'w') as f:\n",
    "    f.write('wpf,wpt,hash\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-thread code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all the subdirectories in the routes directory\n",
    "# date_dirs = os.listdir('../data/osstate/routes') # states_2022-01-03-00.csv\n",
    "# # sort the date_dirs\n",
    "# date_dirs = sorted(date_dirs)\n",
    "# for date_dir in date_dirs[:1]:\n",
    "#     print(f\"Processing {date_dir}\")\n",
    "#     # List all the files in the subdirectory whose filename ends with .zarr_route.csv\n",
    "#     files = os.listdir(f'../data/osstate/routes/{date_dir}')\n",
    "#     files = [f for f in files if f.endswith('.zarr_route.csv')]\n",
    "#     files = sorted(files)\n",
    "#     # files: AEE882_46b823.zarr_route.csv\n",
    "#     for route_file in files[:500]:\n",
    "#         # print(f\"Processing {route_file}\")\n",
    "#         # Open the route file\n",
    "#         route_df = pd.read_csv(f'../data/osstate/routes/{date_dir}/{route_file}')\n",
    "#         # route_df contains two columns: ident and to (time over)\n",
    "#         route_wps = route_df['ident'].values\n",
    "#         if len(route_wps) < 2:\n",
    "#             # print(f\"Warning: Route {route_file} has 0 or 1 waypoint, which is insufficient to determine segments.\")\n",
    "#             continue\n",
    "#         # find lons and lats of route_wps waypoints from the waypoints_master_df\n",
    "#         route_lons = []\n",
    "#         route_lats = []\n",
    "#         for wp in route_wps:\n",
    "#             wp_df = waypoints_master_df[waypoints_master_df['ident'] == wp]\n",
    "#             if len(wp_df) > 0:\n",
    "#                 route_lons.append(wp_df['lon'].values[0])\n",
    "#                 route_lats.append(wp_df['lat'].values[0])\n",
    "\n",
    "#         # extract the segments from the waypoints\n",
    "#         route_normals = get_normal_vectors(np.array(route_lats), np.array(route_lons))\n",
    "#         route_midpoints = get_midpoints(np.array(route_lats), np.array(route_lons))\n",
    "\n",
    "#         # hash the segments\n",
    "#         try:\n",
    "#             normals_hash = hash_with_hpp(route_normals)\n",
    "#             midpoints_hash = compute_segment_position_hash(latlon2xyz(route_midpoints[0], route_midpoints[1]), route_normals)\n",
    "#         except:\n",
    "#             print(f\"Warning: Route {route_file} has an error in hashing the segments.\")\n",
    "#             continue\n",
    "\n",
    "#         # Concatenate the hashes\n",
    "#         route_hash = [f\"{normals_hash[i]}_{midpoints_hash[i]}\" for i in range(len(normals_hash))]\n",
    "\n",
    "#         # Save the route hash\n",
    "#         # Open a new file in ../data/routes_hashes\n",
    "#         with open(f'../data/osstate/routes_hashes/hash_master.hash', 'a') as f:\n",
    "#             for item in route_hash:\n",
    "#                 wp_from = route_wps[route_hash.index(item)]\n",
    "#                 wp_to = route_wps[route_hash.index(item)+1]\n",
    "#                 f.write(f'{wp_from},{wp_to},{item}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "# Hashcode's length parameters\n",
    "segment_bit_length_custom = 480\n",
    "localization_bit_length_custom = 12\n",
    "\n",
    "# Define the function to process each subdirectory\n",
    "def process_subdirectory(date_dir, waypoints_master_df):\n",
    "    # print(f\"Processing {date_dir}\")\n",
    "    # List all the files in the subdirectory whose filename ends with .zarr_route.csv\n",
    "    files = os.listdir(f'../data/osstate/routes/{date_dir}')\n",
    "    files = [f for f in files if f.endswith('.zarr_route.csv')]\n",
    "\n",
    "    hash_file_tag = strip_file_extension(date_dir)\n",
    "\n",
    "    hash_file_writer = open(f'../data/osstate/routes_hashes/hash_master_{hash_file_tag}.hsb', 'w')\n",
    "    \n",
    "    for route_file in files:\n",
    "        # Open the route file\n",
    "        route_df = pd.read_csv(f'../data/osstate/routes/{date_dir}/{route_file}')\n",
    "        # route_df contains two columns: ident and to (time over)\n",
    "        route_wps = route_df['ident'].values\n",
    "        if len(route_wps) < 2:\n",
    "            continue\n",
    "        # find lons and lats of route_wps waypoints from the waypoints_master_df\n",
    "        route_lons = []\n",
    "        route_lats = []\n",
    "        for wp in route_wps:\n",
    "            wp_df = waypoints_master_df[waypoints_master_df['ident'] == wp]\n",
    "            if len(wp_df) > 0:\n",
    "                route_lons.append(wp_df['lon'].values[0])\n",
    "                route_lats.append(wp_df['lat'].values[0])\n",
    "\n",
    "        # extract the segments from the waypoints\n",
    "        route_normals = get_normal_vectors(np.array(route_lats), np.array(route_lons))\n",
    "        route_midpoints = get_midpoints(np.array(route_lats), np.array(route_lons))\n",
    "\n",
    "        # hash the segments\n",
    "        try:\n",
    "            normals_hash = hash_with_hpp(route_normals, num_planes = segment_bit_length_custom)\n",
    "            midpoints_hash = compute_segment_position_hash(latlon2xyz(route_midpoints[0], route_midpoints[1]), route_normals, random_angles = localization_bit_length_custom)\n",
    "        except:\n",
    "            print(f\"Warning: Route {route_file} has an error in hashing the segments.\")\n",
    "            continue\n",
    "\n",
    "        # Concatenate the hashes\n",
    "        route_hash = [f\"{normals_hash[i]}_{midpoints_hash[i]}\" for i in range(len(normals_hash))]\n",
    "\n",
    "        # Save the route hash\n",
    "        \n",
    "        for item in route_hash:\n",
    "            wp_from = route_wps[route_hash.index(item)]\n",
    "            wp_to = route_wps[route_hash.index(item)+1]\n",
    "            hash_file_writer.write(f'{wp_from},{wp_to},{item}\\n')\n",
    "\n",
    "    hash_file_writer.close()\n",
    "\n",
    "# Define the main function to use multiprocessing\n",
    "def main():\n",
    "    # List all the subdirectories in the routes directory\n",
    "    date_dirs = os.listdir('../data/osstate/routes')\n",
    "\n",
    "    # Read the waypoints master dataframe\n",
    "    waypoints_master_df = pd.read_csv(f'../data/osstate/waypoints_master.csv')\n",
    "\n",
    "    # Create a partial function to pass the waypoints_master_df\n",
    "    process_func = partial(process_subdirectory, waypoints_master_df=waypoints_master_df)\n",
    "\n",
    "    # Use multiprocessing to process each subdirectory in parallel\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        pool.map(process_func, date_dirs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/osstate/routes_hashes/hash_master.hashb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def write_hash_master(n_top_hashes: int = 18):\n",
    "    # Open the hash_master file\n",
    "    hash_master_file = open('../data/osstate/routes_hashes/hash_master.hashb', 'w')\n",
    "\n",
    "    # List all .hs files in the routes_hashes directory\n",
    "    hash_files = os.listdir('../data/osstate/routes_hashes')\n",
    "    hash_files = [f for f in hash_files if f.endswith('.hsb')]\n",
    "    hash_files = sorted(hash_files)\n",
    "\n",
    "    for hf in hash_files:\n",
    "        # Open the hash file\n",
    "        hash_file = pd.read_csv(f'../data/osstate/routes_hashes/{hf}', header=None)\n",
    "        hash_file.columns = ['wpf', 'wpt', 'hash']\n",
    "        hash_file_hash = hash_file['hash'].values\n",
    "        # Count the number of times each hash appears\n",
    "        hash_file_count = pd.Series(hash_file_hash).value_counts()\n",
    "        # Sort the hashes by count\n",
    "        hash_file_count = hash_file_count.sort_values(ascending=False)\n",
    "        # Get all the hashes that appear more than once\n",
    "        hash_file_count = hash_file_count[hash_file_count > 1]\n",
    "\n",
    "        if len(hash_file_count) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(hash_file_count) > n_top_hashes:\n",
    "            hash_file_count = hash_file_count[:n_top_hashes]\n",
    "\n",
    "        # Write to hash_master_file all rows in hash_file that have a hash in hash_file_count\n",
    "        for i in range(len(hash_file)):\n",
    "            if hash_file['hash'][i] in hash_file_count:\n",
    "                hash_master_file.write(f\"{hash_file['wpf'][i]},{hash_file['wpt'][i]},{hash_file['hash'][i]}\\n\")\n",
    "\n",
    "    hash_master_file.close()\n",
    "    \n",
    "\n",
    "write_hash_master(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
